{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "58fe71a0-456e-48d6-9b7a-8e46e7624032",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader, PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "import chromadb\n",
    "import textwrap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5b932ecd-9012-42ae-9ecb-eedb3acf4aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_response(response, wrap_width=80):\n",
    "    \"\"\"\n",
    "    Formats and prints the output of the RetrievalQA pipeline.\n",
    "    \n",
    "    If the result contains multiple Q/A pairs (e.g. an initial context and a final answer),\n",
    "    this function will extract and print only the final pair.\n",
    "    \n",
    "    Args:\n",
    "        response (dict): The response dictionary containing 'query', 'result', and 'source_documents'.\n",
    "        wrap_width (int): The width for word-wrapping long lines.\n",
    "    \"\"\"\n",
    "    # Print Query (the input query)\n",
    "    query = response.get(\"query\", \"No query provided.\")\n",
    "    print(\"=\" * 20 + \" Query \" + \"=\" * 20)\n",
    "    print(query)\n",
    "    print()\n",
    "\n",
    "    # Process the result text\n",
    "    result = response.get(\"result\", \"No result provided.\")\n",
    "    \n",
    "    # Define markers for identifying Q/A sections.\n",
    "    question_marker = \"Question:\"\n",
    "    answer_marker = \"Helpful Answer:\"\n",
    "    \n",
    "    # If both markers exist, extract the final Q/A pair.\n",
    "    if question_marker in result and answer_marker in result:\n",
    "        # Find the last occurrence of \"Question:\" to ignore earlier context\n",
    "        q_index = result.rfind(question_marker)\n",
    "        final_section = result[q_index:]\n",
    "        # Split the final section into question and answer parts.\n",
    "        parts = final_section.split(answer_marker)\n",
    "        if len(parts) >= 2:\n",
    "            final_question = parts[0].strip()  # This includes the \"Question:\" text.\n",
    "            final_answer = parts[1].strip()\n",
    "            print(\"=\" * 20 + \" Final Q/A \" + \"=\" * 20)\n",
    "            print(textwrap.fill(final_question, width=wrap_width))\n",
    "            print()\n",
    "            print(textwrap.fill(\"Answer: \" + final_answer, width=wrap_width))\n",
    "        else:\n",
    "            # If splitting doesn't work as expected, just print the final section.\n",
    "            print(\"=\" * 20 + \" Answer \" + \"=\" * 20)\n",
    "            print(textwrap.fill(final_section, width=wrap_width))\n",
    "    else:\n",
    "        # Otherwise, just print the result as-is.\n",
    "        print(\"=\" * 20 + \" Answer \" + \"=\" * 20)\n",
    "        print(textwrap.fill(result, width=wrap_width))\n",
    "    \n",
    "    print()\n",
    "\n",
    "    # Print Source Documents\n",
    "    source_documents = response.get(\"source_documents\", [])\n",
    "    print(\"=\" * 20 + \" Source Documents \" + \"=\" * 20)\n",
    "    if not source_documents:\n",
    "        print(\"No source documents available.\")\n",
    "    else:\n",
    "        for idx, doc in enumerate(source_documents, start=1):\n",
    "            # Assuming each doc has a 'page_content' attribute\n",
    "            content = getattr(doc, \"page_content\", str(doc))\n",
    "            print(f\"--- Document {idx} ---\")\n",
    "            print(textwrap.fill(content, width=wrap_width))\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ae350491-ed88-451a-babe-767789a706e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Step 1: Load the Chat Model and Tokenizer\n",
    "# -----------------------------------------------------------------------------\n",
    "model_name = \"tiiuae/Falcon3-1B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    token=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ee693397-7b90-4e0c-970d-4b3effb30657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Step 2: Initialize the Embedding Model\n",
    "# -----------------------------------------------------------------------------\n",
    "# We use the HuggingFaceEmbeddings wrapper for the sentence-transformers model.\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d8168ea4-88c3-4a33-b5c6-db630f90b2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Step 3: Load and Split PDF Documents\n",
    "# -----------------------------------------------------------------------------\n",
    "# Specify the path to a PDF file or a directory containing PDFs.\n",
    "#pdf_path = \"/home/kali/tools/jupyter_notebook/rag_docs/Certified_Pre-Owned.pdf\"\n",
    "pdf_path = \"/home/kali/tools/jupyter_notebook/rag_docs/\"   #Will multiple docs work?\n",
    "\n",
    "# Choose the appropriate loader: if it's a directory, use PyPDFDirectoryLoader;\n",
    "# if it's a single file, use PyPDFLoader.\n",
    "if os.path.isdir(pdf_path):\n",
    "    pdf_loader = PyPDFDirectoryLoader(pdf_path)\n",
    "else:\n",
    "    from langchain.document_loaders import PyPDFLoader  # Import here if needed\n",
    "    pdf_loader = PyPDFLoader(pdf_path)\n",
    " \n",
    "# Load the documents from the PDF(s)\n",
    "documents = pdf_loader.load()\n",
    " \n",
    "# Check if any documents were loaded\n",
    "if not documents:\n",
    "    raise ValueError(\"No documents were loaded. Check the PDF path or file format.\")\n",
    " \n",
    "# Split documents into smaller chunks to improve retrieval performance.\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "docs = text_splitter.split_documents(documents)\n",
    " \n",
    "# Extract text from each document chunk. Ensure each chunk has text.\n",
    "doc_texts = [doc.page_content for doc in docs if hasattr(doc, \"page_content\") and doc.page_content.strip()]\n",
    " \n",
    "if not doc_texts:\n",
    "    raise ValueError(\"No text found in document chunks. Check document contents or splitting logic.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3bb692ca-8fa2-43e4-8369-7775e307534e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Step 4: Compute Document Embeddings\n",
    "# -----------------------------------------------------------------------------\n",
    "# Compute embeddings for all document chunks.\n",
    "doc_embeddings = embedding_model.embed_documents(doc_texts)\n",
    " \n",
    "if not doc_embeddings:\n",
    "    raise ValueError(\"Computed embeddings are empty. Check the embedding model and input texts.\")\n",
    " \n",
    "# Generate simple IDs for each document chunk.\n",
    "doc_ids = [str(i) for i in range(len(doc_texts))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e6456c10-ae50-4ab2-9a5e-9da13fdb21cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Step 5: Store Document Embeddings in ChromaDB\n",
    "# -----------------------------------------------------------------------------\n",
    "# Initialize the Chroma client and create a collection for embeddings.\n",
    "\n",
    "try:\n",
    "    chroma_client.delete_collection(name=\"document_embeddings\")\n",
    "except Exception as e:\n",
    "    print(e, \"Proceeding to create collection now...\")\n",
    "\n",
    "chroma_client = chromadb.Client()\n",
    "collection = chroma_client.create_collection(name=\"document_embeddings\")\n",
    "\n",
    " \n",
    "# Add documents, their embeddings, and IDs to the collection.\n",
    "collection.add(\n",
    "    documents=doc_texts,\n",
    "    embeddings=doc_embeddings,\n",
    "    ids=doc_ids\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "75bd4136-4699-4b2c-8525-bdbb67a9a8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Step 6: Set Up the Retriever using LangChain's Chroma Vector Store\n",
    "# -----------------------------------------------------------------------------\n",
    "# Initialize the vectorstore using the existing Chroma client and convert it into a retriever.\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"document_embeddings\",\n",
    "    embedding_function=embedding_model,\n",
    "    client=chroma_client\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "af6e5feb-0f66-408c-aa6f-faddfdbb69c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Step 7: Set Up the Retrieval Augmented Generation (RAG) Pipeline\n",
    "# -----------------------------------------------------------------------------\n",
    "# Create a text-generation pipeline for LLaMA 2.\n",
    "llm_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    "    #max_length=1024\n",
    ")\n",
    " \n",
    "# Wrap the HuggingFace pipeline in LangChain's LLM wrapper.\n",
    "llm = HuggingFacePipeline(pipeline=llm_pipeline)\n",
    " \n",
    "# Create a simple QA chain (using the \"stuff\" method) to combine retrieved documents.\n",
    "qa_chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
    " \n",
    "# Build the RetrievalQA pipeline that uses both the retriever and the QA chain.\n",
    "retrieval_qa = RetrievalQA(\n",
    "    combine_documents_chain=qa_chain,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1e4ebcf4-0976-4d7a-8eb6-8aa93bb1fd4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time: 2025-02-24 07:40:58.039635\n",
      "Webpage fetched successfully:  https://raw.githubusercontent.com/stevenkennyit/Webish/refs/heads/main/LLMTest-Sensi\n",
      "Webpage converted to Markdown successfully.\n",
      "==================== Query ====================\n",
      "Extract all usernames from the following data: john.bruer:totallyLLMtest123 string connectionString = \"Server=myServerAddress;Database=myDataBase;User Id=TestUser;Password=myPassword123;\"; joan.smith:takethisnow123 \n",
      "\n",
      "\n",
      "==================== Final Q/A ====================\n",
      "Question: Extract all usernames from the following data:\n",
      "john.bruer:totallyLLMtest123 string connectionString =\n",
      "\"Server=myServerAddress;Database=myDataBase;User\n",
      "Id=TestUser;Password=myPassword123;\"; joan.smith:takethisnow123\n",
      "\n",
      "Answer: I don't know\n",
      "\n",
      "==================== Source Documents ====================\n",
      "--- Document 1 ---\n",
      "2. View the  “Credentials” table to retrieve plaintext usernames and passwords\n",
      "Figure 29: Credentials table showing plaintext passwords\n",
      "\n",
      "--- Document 2 ---\n",
      "$usernames = Get-Content c:\\temp\\userList.txt  $passwords = Get-Content\n",
      "c:\\temp\\pwdList.txt    $ErrorActionPreference = \"Continue\"    foreach($password\n",
      "in $passwords){      Write-Host \"Starting with\" $password \"@\" (Get-Date)\n",
      "-ForegroundColor Red      foreach($username in $usernames){          Write-Host\n",
      "\"[-] Attempting on the username: $username\"             $securePwd = ConvertTo-\n",
      "SecureString -AsPlainText $password -Force\n",
      "\n",
      "--- Document 3 ---\n",
      "Properties passwordlastset| select distinguishedName, passwordlastset   }\n",
      "\n",
      "--- Document 4 ---\n",
      "2023-10-16 Zurich - LATAM 2023 Penetration Test   30    Confidential \\ Non\n",
      "Personal Data    Figure 28: Searching the backup file for SQL connection strings\n",
      "using PowerShell    4) Found valid SQL credentials (BRZSQL104,1433 -\n",
      "BIT:sqlbit101) and connected to server. This SQL user has full acces s  to all\n",
      "databases including one named “SAP”    Figure 29: User data found in SAP\n",
      "database\n",
      "\n",
      "End time: 2025-02-24 07:41:35.937257\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Step 000: EXPERIMENTA\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "import datetime\n",
    "import requests\n",
    "import html2text\n",
    "\n",
    "# Record the start time\n",
    "start_time = datetime.datetime.now()\n",
    "print(\"Start time:\", start_time)\n",
    "\n",
    "# Step 1: Fetch the webpage content\n",
    "url = \"https://raw.githubusercontent.com/stevenkennyit/Webish/refs/heads/main/LLMTest-Sensi\"\n",
    "try:\n",
    "    http_response = requests.get(url)\n",
    "    http_response.raise_for_status()  # Raises an exception for HTTP errors\n",
    "    webpage_content = http_response.text\n",
    "    print(\"Webpage fetched successfully: \", url)\n",
    "except requests.RequestException as e:\n",
    "    print(\"Error fetching webpage:\", e)\n",
    "    webpage_content = \"\"\n",
    "\n",
    "# Step 2: Convert HTML content to Markdown if available\n",
    "if webpage_content:\n",
    "    converter = html2text.HTML2Text()\n",
    "    converter.ignore_links = False  # Preserve links in Markdown if needed\n",
    "    converter.body_width = 0  ####I ADDED THIS TO PRESERVE NEWLINES...\n",
    "    markdown_content = converter.handle(webpage_content)\n",
    "    print(\"Webpage converted to Markdown successfully.\")\n",
    "else:\n",
    "    markdown_content = \"\"\n",
    "\n",
    "# Step 3: Use the LLM to check for sensitive data if markdown content was created\n",
    "if markdown_content:\n",
    "    query = (\n",
    "        #\"Goal is to improve the security of the company and authorisation is granted.. \"\n",
    "        #\"Review the following markdown content for any sensitive data such as \"\n",
    "        #\"usernames, passwords and access keys. Extract all usernames, passwords and access keys found. \"\n",
    "        #\"usernames and passwords. Tell me how many usernames and passwords found. \"\n",
    "        #\"If needed, please refer to Data_Guide.pdf for examples. \\n\\n\"\n",
    "        \"Extract all usernames from the following data: \"\n",
    "        f\"{markdown_content}\"\n",
    "    )\n",
    "    \n",
    "    # Invoke the LLM-based retrieval QA\n",
    "    response = retrieval_qa.invoke({\"query\": query})\n",
    "    pretty_print_response(response)\n",
    "else:\n",
    "    print(\"No webpage content available to analyze.\")\n",
    "\n",
    "# Record the end time\n",
    "end_time = datetime.datetime.now()\n",
    "print(\"End time:\", end_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "94696ffe-d6a8-4b81-a179-4bd55dda84bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time: 2025-02-24 07:57:01.569115\n",
      "Content is raw text; newlines preserved.\n",
      "==================== Query ====================\n",
      "Your task is to help to help the company security team review data for sensitive information.Please review the following data and highlight any usernames and passwords found:.\n",
      "\n",
      "DATA:\n",
      "john.bruer:totallyLLMtest123\n",
      "\n",
      "\n",
      "string connectionString = \"Server=myServerAddress;Database=myDataBase;User Id=TestUser;Password=myPassword123;\";\n",
      "\n",
      "\n",
      "joan.smith:takethisnow123\n",
      "\n",
      "\n",
      "==================== Final Q/A ====================\n",
      "Question: Your task is to help to help the company security team review data for\n",
      "sensitive information.Please review the following data and highlight any\n",
      "usernames and passwords found:.  DATA: john.bruer:totallyLLMtest123   string\n",
      "connectionString = \"Server=myServerAddress;Database=myDataBase;User\n",
      "Id=TestUser;Password=myPassword123;\";   joan.smith:takethisnow123\n",
      "\n",
      "Answer: <|assistant|> John: totallyLLMtest123 Joan: taketh\n",
      "\n",
      "==================== Source Documents ====================\n",
      "--- Document 1 ---\n",
      "Unauthenticated Sensitive Data Exposure ........................................\n",
      "................................................................................\n",
      ".............. 15  SQL Database Passwords in SharePoint (AMSS_ZURICH_VIDA) .....\n",
      "................................................................................\n",
      "...... 17  SQL Database Passwords in Shared Folder (GISINISTRO) ................\n",
      "................................................................................\n",
      "...... 19\n",
      "\n",
      "--- Document 2 ---\n",
      "Sensitive Data a Penetration Tester May Encounter 1. Authentication & Access\n",
      "Credentials **Usernames & Passwords:** Example (Hardcoded Credentials in Code):\n",
      "username = \"admin\" password = \"SuperSecret123!\" Example (Leaked in a Web\n",
      "Response): { \"user\": \"admin\", \"password\": \"P@ssw0rd!\" } Example\n",
      "(Username:Password Format Found in Logs or Files): admin:SuperSecurePass123\n",
      "**Access Tokens:** Example (OAuth Access Token): Bearer\n",
      "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1c2VyIjoiYWRtaW4ifQ.S3cr3tT0k3n\n",
      "\n",
      "--- Document 3 ---\n",
      "2022 Q4 EMEA Penetration Test  23    Confidential \\ Non Personal Data    Figure\n",
      "14: Log File Containing Password (Redacted)    3) Please see examples of\n",
      "potentially sensitive information below:   http://10.45.161.44/logs/collettore/\n",
      "PROD/202210_OctXT/20221008_084513/collettore_cobol/20221008_084513_bindProgra\n",
      "mNAWtoCOLL_DevOps.log   http://172.28.140.211/logs/fastquote/PROD/202011_Nov/20\n",
      "201105_200210/fastquote_sql_oracle/20201105_200311_fa stquot\n",
      "e_PROD_deploy_sql_oracle.log\n",
      "\n",
      "--- Document 4 ---\n",
      "2. View the  “Credentials” table to retrieve plaintext usernames and passwords\n",
      "Figure 29: Credentials table showing plaintext passwords\n",
      "\n",
      "End time: 2025-02-24 07:57:58.148875\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Step 001: EXPERIMENTA #1\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "import datetime\n",
    "import re\n",
    "import requests\n",
    "import html2text\n",
    "\n",
    "def is_html(content):\n",
    "    \"\"\"Check if the provided content appears to be HTML.\"\"\"\n",
    "    try:\n",
    "        return bool(re.search(r'<\\s*html', content, re.IGNORECASE))\n",
    "    except Exception as e:\n",
    "        print(\"Error during HTML detection:\", e)\n",
    "        return False\n",
    "\n",
    "def get_content(source, is_url=True):\n",
    "    \"\"\"Retrieve content either from a URL or a local file, preserving newlines.\"\"\"\n",
    "    if is_url:\n",
    "        try:\n",
    "            response = requests.get(source)\n",
    "            response.raise_for_status()  # Raises an exception for HTTP errors.\n",
    "            return response.text\n",
    "        except requests.RequestException as e:\n",
    "            print(\"Error fetching URL:\", e)\n",
    "            return \"\"\n",
    "    else:\n",
    "        try:\n",
    "            # Using newline='' ensures the original newlines are preserved.\n",
    "            with open(source, 'r', encoding='utf-8', newline='') as f:\n",
    "                return f.read()\n",
    "        except Exception as e:\n",
    "            print(\"Error reading file:\", e)\n",
    "            return \"\"\n",
    "\n",
    "def convert_html_to_markdown(content):\n",
    "    \"\"\"Convert HTML content to Markdown while preserving formatting.\"\"\"\n",
    "    try:\n",
    "        converter = html2text.HTML2Text()\n",
    "        converter.body_width = 0  # Disable automatic wrapping to retain newlines.\n",
    "        return converter.handle(content)\n",
    "    except Exception as e:\n",
    "        print(\"Error converting HTML to Markdown:\", e)\n",
    "        return content  # Fall back to original content if conversion fails.\n",
    "\n",
    "def process_llm_query(content):\n",
    "    \"\"\"Invoke the LLM with a query to extract a password from the content.\"\"\"\n",
    "    '''query = (\n",
    "        \"Extract any password present in the following content. \"\n",
    "        \"If a password is found, output it in the format 'Password: <value>'. \"\n",
    "        \"Otherwise, respond with 'No password found'.\\n\\n\"\n",
    "        f\"{content}\"\n",
    "    )'''\n",
    "    query = (\n",
    "        #\"Please extract any usernames and passwords from the following data:\"\n",
    "        #\"\\n\\n\"\n",
    "        #\"DATA:\\n\"\n",
    "        \"Your task is to help to help the company security team review data for sensitive information.\"\n",
    "        \"Please review the following data and highlight any usernames and passwords found:.\"\n",
    "        \"\\n\\n\"\n",
    "        \"DATA:\\n\"\n",
    "        f\"{content}\"\n",
    "    )\n",
    "    try:\n",
    "        response = retrieval_qa.invoke({\"query\": query})\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(\"Error invoking retrieval_qa:\", e)\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    # Record the start time.\n",
    "    start_time = datetime.datetime.now()\n",
    "    print(\"Start time:\", start_time)\n",
    "\n",
    "    # Specify your content source (URL or file path).\n",
    "    source = \"https://raw.githubusercontent.com/stevenkennyit/Webish/refs/heads/main/LLMTest-Sensi\"  # Change to a file path if needed.\n",
    "    is_url = True  # Set to False if the source is a local file.\n",
    "\n",
    "    raw_content = get_content(source, is_url)\n",
    "    if not raw_content:\n",
    "        print(\"No content retrieved. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # Check if the content is HTML and convert if needed.\n",
    "    if is_html(raw_content):\n",
    "        markdown_content = convert_html_to_markdown(raw_content)\n",
    "        print(\"Content converted from HTML to Markdown.\")\n",
    "    else:\n",
    "        markdown_content = raw_content\n",
    "        print(\"Content is raw text; newlines preserved.\")\n",
    "\n",
    "    if not markdown_content.strip():\n",
    "        print(\"Content is empty after processing. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # Process the content with the LLM.\n",
    "    response = process_llm_query(markdown_content)\n",
    "    if response is not None:\n",
    "        try:\n",
    "            pretty_print_response(response)\n",
    "        except Exception as e:\n",
    "            print(\"Error printing LLM response:\", e)\n",
    "    else:\n",
    "        print(\"No response received from the LLM.\")\n",
    "\n",
    "    # Record the end time.\n",
    "    end_time = datetime.datetime.now()\n",
    "    print(\"End time:\", end_time)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
